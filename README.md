# CSV to Parquet Conversion â€“ Performance & Storage Optimization

This small project is a hands-on experiment focused on **converting large CSV files into Parquet format** using Python and Pandas, with the goal of improving:

- File size (storage optimization)
- Read speed (performance benchmarking)

---

## ğŸ” What I Learned

While working on this, I wanted to explore how Parquet â€” as a columnar, compressed file format â€” compares to traditional CSV files in practical terms.

Key insights:
- Parquet significantly reduces file size (up to 76% in my tests)
- It improves read time by ~50%
- Not all datasets compress equally â€” data types and structure matter
- Converting back to CSV ("de-parquetizing") often results in even larger files due to expansion into plain text

---

## ğŸ“ Project Structure

Parquet/
â”œâ”€â”€ datasets/
â”‚ â””â”€â”€ flights.csv # Sample dataset (not uploaded due to size, but you can find it in this: https://www.kaggle.com/datasets/usdot/flight-delays?resource=download)
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ convert_to_parquet.py # Converts CSV to Parquet
â”‚ â”œâ”€â”€ compare_times_and_sizes.py # Benchmarks read speed and size differences
â”‚ â””â”€â”€ revert_to_csv.py # Converts Parquet back to CSV

## âš™ï¸ How to Run

1. Clone the repository:
 ``` bash
git clone https://github.com/MauroValerioti/Parquetization_with_python.git
cd parquet-experiment

2. Place your CSV file inside the datasets/ folder.

3. Run the conversion script:
python scripts/convert_to_parquet.py

4. Run the benchmarking script:
python scripts/compare_times_and_sizes.py

5. Optionally, revert to CSV:
python scripts/revert_to_csv.py
```

ğŸ“¦ Tools & Libraries
- Python 3.10+
- pandas
- pyarrow
- time
- os / pathlib

ğŸ§ª Example Results
- Original CSV size: 578 MB
- Converted Parquet size: 136 MB
- Compression ratio: ~76%
- CSV read time: 0.88s
- Parquet read time: 0.43s

ğŸ§  Why This Matters
Working with large datasets is common in data pipelines, analytics, and ETL processes. Learning to optimize how data is stored and read can have a real impact on speed, cost, and scalability.
This was a quick, practical way to understand how formats like Parquet can make a difference.

## ğŸ“¬ Contact

Feel free to reach out via [LinkedIn](https://www.linkedin.com/in/maurovalerioti) if you have feedback or want to connect.
